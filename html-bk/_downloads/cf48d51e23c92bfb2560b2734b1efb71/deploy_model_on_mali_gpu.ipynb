{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nDeploy the Pretrained Model on ARM Mali GPU\n===========================================\n**Author**: `Lianmin Zheng <https://lmzheng.net/>`_, `Ziheng Jiang <https://ziheng.org/>`_\n\nThis is an example of using NNVM to compile a ResNet model and\ndeploy it on Firefly-RK3399 with ARM Mali GPU. We will use the\nMali-T860 MP4 GPU on this board to accelerate the inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nimport nnvm.compiler\nimport nnvm.testing\nfrom tvm import rpc\nfrom tvm.contrib import util, graph_runtime as runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build TVM Runtime on Device\n---------------------------\n\nThe first step is to build tvm runtime on the remote device.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>All instructions in both this section and next section should be\n  executed on the target device, e.g. Rk3399. And we assume it\n  has Linux running.</p></div>\n\nSince we do compilation on local machine, the remote device is only used\nfor running the generated code. We only need to build tvm runtime on\nthe remote device. Make sure you have opencl driver in your board.\nYou can refer to `tutorial <https://gist.github.com/mli/585aed2cec0b5178b1a510f9f236afa2>`_\nto setup OS and opencl driver for rk3399.\n\n.. code-block:: bash\n\n  git clone --recursive https://github.com/dmlc/tvm\n  cd tvm\n  cp cmake/config.cmake .\n  sed -i \"s/USE_OPENCL OFF/USE_OPENCL ON/\" config.cmake \n  make runtime -j4\n\nAfter building runtime successfully, we need to set environment varibles\nin :code:`~/.bashrc` file. We can edit :code:`~/.bashrc`\nusing :code:`vi ~/.bashrc` and add the line below (Assuming your TVM \ndirectory is in :code:`~/tvm`):\n\n.. code-block:: bash\n\n  export PYTHONPATH=$PYTHONPATH:~/tvm/python\n\nTo update the environment variables, execute :code:`source ~/.bashrc`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Up RPC Server on Device\n---------------------------\nTo start an RPC server, run the following command on your remote device\n(Which is RK3399 in our example).\n\n  .. code-block:: bash\n\n    python -m tvm.exec.rpc_server --host 0.0.0.0 --port=9090\n\nIf you see the line below, it means the RPC server started\nsuccessfully on your device.\n\n   .. code-block:: bash\n\n     INFO:root:RPCServer: bind to 0.0.0.0:9090\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the Pre-trained Model\n-----------------------------\nBack to the host machine, which should have a full TVM installed (with LLVM).\n\nWe will use pre-trained model from\n`MXNet Gluon model zoo <https://mxnet.incubator.apache.org/api/python/gluon/model_zoo.html>`_.\nYou can found more details about this part at tutorial `tutorial-from-mxnet`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mxnet.gluon.model_zoo.vision import get_model\nfrom mxnet.gluon.utils import download\nfrom PIL import Image\nimport numpy as np\n\n# only one line to get the model\nblock = get_model('resnet18_v1', pretrained=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to test our model, here we download an image of cat and\ntransform its format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_name = 'cat.jpg'\ndownload('https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true', img_name)\nimage = Image.open(img_name).resize((224, 224))\n\ndef transform_image(image):\n    image = np.array(image) - np.array([123., 117., 104.])\n    image /= np.array([58.395, 57.12, 57.375])\n    image = image.transpose((2, 0, 1))\n    image = image[np.newaxis, :]\n    return image\n\nx = transform_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "synset is used to transform the label from number of ImageNet class to\nthe word human can understand.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "synset_url = ''.join(['https://gist.githubusercontent.com/zhreshold/',\n                      '4d0b62f3d01426887599d4f7ede23ee5/raw/',\n                      '596b27d23537e5a1b5751d2b0481ef172f58b539/',\n                      'imagenet1000_clsid_to_human.txt'])\n\nsynset_name = 'synset.txt'\ndownload(synset_url, synset_name)\nwith open(synset_name) as f:\n    synset = eval(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we would like to port the Gluon model to a portable computational graph.\nIt's as easy as several lines.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We support MXNet static graph(symbol) and HybridBlock in mxnet.gluon\nnet, params = nnvm.frontend.from_mxnet(block)\n# we want a probability so add a softmax operator\nnet = nnvm.sym.softmax(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here are some basic data workload configurations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 1\nnum_classes = 1000\nimage_shape = (3, 224, 224)\ndata_shape = (batch_size,) + image_shape\nout_shape = (batch_size, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile The Graph\n-----------------\nTo compile the graph, we call the :any:`nnvm.compiler.build` function\nwith the graph configuration and parameters. As we use OpenCL for\nGPU computing, the tvm will generate both OpenCL kernel code and ARM\nCPU host code. The CPU host code is used for calling OpenCL kernels.\nIn order to generate correct CPU code, we need to specify the target\ntriplet for host ARM device by setting the parameter :code:`target_host`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we run the example on our x86 server for demonstration, we can simply\nset it as :code:`llvm`. If running it on the RK3399, we need to\nspecify its instruction set. Set :code:`local_demo` to False if you\nwant to run this tutorial with a real device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_demo = True\n\nif local_demo:\n    target_host = \"llvm\"\n    target = \"llvm\"\nelse:\n    # Here is the setting for my rk3399 board\n    # If you don't use rk3399, you can query your target triple by \n    # execute `gcc -v` on your board.\n    target_host = \"llvm -target=aarch64-linux-gnu\"\n\n    # set target as  `tvm.target.mali` instead of 'opencl' to enable\n    # optimization for mali\n    target = tvm.target.mali()\n\nwith nnvm.compiler.build_config(opt_level=3):\n    graph, lib, params = nnvm.compiler.build(net, target=target,\n            shape={\"data\": data_shape}, params=params, target_host=target_host)\n\n# After `nnvm.compiler.build`, you will get three return values: graph,\n# library and the new parameter, since we do some optimization that will\n# change the parameters but keep the result of model as the same.\n\n# Save the library at local temporary directory.\ntmp = util.tempdir()\nlib_fname = tmp.relpath('net.tar')\nlib.export_library(lib_fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy the Model Remotely by RPC\n--------------------------------\nWith RPC, you can deploy the model remotely from your host machine\nto the remote device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# obtain an RPC session from remote device.\nif local_demo:\n    remote = rpc.LocalSession()\nelse:\n    # The following is my environment, change this to the IP address of your target device\n    host = '10.77.1.145'\n    port = 9090\n    remote = rpc.connect(host, port)\n\n# upload the library to remote device and load it\nremote.upload(lib_fname)\nrlib = remote.load_module('net.tar')\n\nctx = remote.cpu(0) if local_demo else remote.cl(0)\n# upload the parameter\nrparams = {k: tvm.nd.array(v, ctx) for k, v in params.items()}\n\n# create the remote runtime module\nmodule = runtime.create(graph, rlib, ctx)\n# set parameter\nmodule.set_input(**rparams)\n# set input data\nmodule.set_input('data', tvm.nd.array(x.astype('float32')))\n# run\nmodule.run()\n# get output\nout = module.get_output(0, tvm.nd.empty(out_shape, ctx=ctx))\n# get top1 result\ntop1 = np.argmax(out.asnumpy())\nprint('TVM prediction top-1: {}'.format(synset[top1]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}