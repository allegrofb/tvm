

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Get Started with NNVM &mdash; tvm 0.5.dev documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tvm_theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Compile MXNet Models" href="from_mxnet.html" />
    <link rel="prev" title="Compile ONNX Models" href="from_onnx.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.5.dev
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../nnvm_quick_start.html">Quick Start Tutorial for Compiling Deep Learning Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started.html">Get Started with TVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#auto-tuning">Auto tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#compile-deep-learning-models">Compile Deep Learning Models</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="using_external_lib.html">Using External Libraries in NNVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_coreml.html">Compile CoreML Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_onnx.html">Compile ONNX Models</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Get Started with NNVM</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#declare-computation">Declare Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#compile">Compile</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deploy-and-run">Deploy and Run</a></li>
<li class="toctree-l4"><a class="reference internal" href="#provide-model-parameters">Provide Model Parameters</a></li>
<li class="toctree-l4"><a class="reference internal" href="#save-the-deployed-module">Save the Deployed Module</a></li>
<li class="toctree-l4"><a class="reference internal" href="#deploy-using-another-language">Deploy using Another Language</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="from_mxnet.html">Compile MXNet Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_keras.html">Compile Keras Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="deploy_model_on_rasp.html">Deploy the Pretrained Model on Raspberry Pi</a></li>
<li class="toctree-l3"><a class="reference internal" href="deploy_model_on_mali_gpu.html">Deploy the Pretrained Model on ARM Mali GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_darknet.html">Compile YOLO-V2 in DarkNet Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="deploy_ssd.html">Deploy Single Shot Multibox Detector(SSD) model</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_tensorflow.html">Compile Tensorflow Models</a></li>
<li class="toctree-l3"><a class="reference internal" href="from_mxnet_to_webgl.html">Deploy Deep Learning Models to OpenGL and WebGL</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy/index.html">Deploy and Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_links.html">Links to C++ and JS API References</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index.html">Design and Developer Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../nnvm_top.html">NNVM Core Tensor Operators</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">tvm</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
      <li>Get Started with NNVM</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/nnvm/get_started.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-nnvm-get-started-py"><span class="std std-ref">here</span></a>     to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="get-started-with-nnvm">
<span id="sphx-glr-tutorials-nnvm-get-started-py"></span><h1>Get Started with NNVM<a class="headerlink" href="#get-started-with-nnvm" title="Permalink to this headline">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://tqchen.github.io/">Tianqi Chen</a></p>
<p>This article is an introductory tutorial to workflow in NNVM.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">nnvm.compiler</span>
<span class="kn">import</span> <span class="nn">nnvm.symbol</span> <span class="k">as</span> <span class="nn">sym</span>
</pre></div>
</div>
<div class="section" id="declare-computation">
<h2>Declare Computation<a class="headerlink" href="#declare-computation" title="Permalink to this headline">¶</a></h2>
<p>We start by describing our need using computational graph.
Most deep learning frameworks use computation graph to describe
their computation. In this example, we directly use
NNVM’s API to construct the computational graph.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In a typical deep learning compilation workflow,
we can get the models from <a class="reference internal" href="../../api/python/nnvm/frontend.html#module-nnvm.frontend" title="nnvm.frontend"><code class="xref any py py-mod docutils literal notranslate"><span class="pre">nnvm.frontend</span></code></a></p>
</div>
<p>The following code snippet describes <span class="math notranslate nohighlight">\(z = x + \sqrt{y}\)</span>
and creates a nnvm graph from the description.
We can print out the graph ir to check the graph content.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">sym</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sym</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">sym</span><span class="o">.</span><span class="n">elemwise_add</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sym</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="n">compute_graph</span> <span class="o">=</span> <span class="n">nnvm</span><span class="o">.</span><span class="n">graph</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------compute graph-------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compute_graph</span><span class="o">.</span><span class="n">ir</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="compile">
<h2>Compile<a class="headerlink" href="#compile" title="Permalink to this headline">¶</a></h2>
<p>We can call <a class="reference internal" href="../../api/python/nnvm/compiler.html#nnvm.compiler.build" title="nnvm.compiler.build"><code class="xref any py py-func docutils literal notranslate"><span class="pre">nnvm.compiler.build</span></code></a> to compile the graph.
The build function takes a shape parameter which specifies the
input shape requirement. Here we only need to pass in shape of <code class="docutils literal notranslate"><span class="pre">x</span></code>
and the other one will be inferred automatically by NNVM.</p>
<p>The function returns three values. <code class="docutils literal notranslate"><span class="pre">deploy_graph</span></code> contains
the final compiled graph structure. <code class="docutils literal notranslate"><span class="pre">lib</span></code> is a <a class="reference internal" href="../../api/python/module.html#tvm.module.Module" title="tvm.module.Module"><code class="xref any py py-class docutils literal notranslate"><span class="pre">tvm.module.Module</span></code></a>
that contains compiled CUDA functions. We do not need the <code class="docutils literal notranslate"><span class="pre">params</span></code>
in this case.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,)</span>
<span class="n">deploy_graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">nnvm</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">compute_graph</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">shape</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>We can print out the IR of <code class="docutils literal notranslate"><span class="pre">deploy_graph</span></code> to understand what just
happened under the hood. We can find that <code class="docutils literal notranslate"><span class="pre">deploy_graph</span></code> only
contains a single operator <code class="docutils literal notranslate"><span class="pre">tvm_op</span></code>. This is because NNVM
automatically fused the operator together into one operator.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------deploy graph-------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">deploy_graph</span><span class="o">.</span><span class="n">ir</span><span class="p">())</span>
</pre></div>
</div>
<p>Let us also peek into content of <code class="docutils literal notranslate"><span class="pre">lib</span></code>.
Typically a compiled TVM CUDA module contains a host module(lib)
and a device module(<code class="docutils literal notranslate"><span class="pre">lib.imported_modules[0]</span></code>) that contains the CUDA code.
We print out the the generated device code here.
This is exactly a fused CUDA version of kernel that the graph points to.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------deploy library-------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lib</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="deploy-and-run">
<h2>Deploy and Run<a class="headerlink" href="#deploy-and-run" title="Permalink to this headline">¶</a></h2>
<p>Now that we have have compiled module, let us run it.
We can use <a class="reference internal" href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.create" title="tvm.contrib.graph_runtime.create"><code class="xref any py py-func docutils literal notranslate"><span class="pre">graph_runtime</span></code></a>
in tvm to create a deployable <a class="reference internal" href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule" title="tvm.contrib.graph_runtime.GraphModule"><code class="xref any py py-class docutils literal notranslate"><span class="pre">GraphModule</span></code></a>.
We can use the <a class="reference internal" href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule.set_input" title="tvm.contrib.graph_runtime.GraphModule.set_input"><code class="xref any py py-meth docutils literal notranslate"><span class="pre">set_input</span></code></a>,
<a class="reference internal" href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule.run" title="tvm.contrib.graph_runtime.GraphModule.run"><code class="xref any py py-meth docutils literal notranslate"><span class="pre">run</span></code></a> and
<a class="reference internal" href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.GraphModule.get_output" title="tvm.contrib.graph_runtime.GraphModule.get_output"><code class="xref any py py-meth docutils literal notranslate"><span class="pre">get_output</span></code></a> function
to set the input, execute the graph and get the output we need.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">graph_runtime</span><span class="p">,</span> <span class="n">util</span>

<span class="n">module</span> <span class="o">=</span> <a href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.create" title="tvm.contrib.graph_runtime.create" class="sphx-glr-backref-module-tvm-contrib-graph_runtime sphx-glr-backref-type-py-function"><span class="n">graph_runtime</span><span class="o">.</span><span class="n">create</span></a><span class="p">(</span><span class="n">deploy_graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <a href="../../api/python/ndarray.html#tvm.gpu" title="tvm.gpu" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">gpu</span></a><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">x_np</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.9.1/reference/generated/numpy.html#numpy.array" title="numpy.array" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-np-function"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">y_np</span> <span class="o">=</span> <a href="http://docs.scipy.org/doc/numpy-1.9.1/reference/generated/numpy.html#numpy.array" title="numpy.array" class="sphx-glr-backref-module-numpy sphx-glr-backref-type-np-function"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="c1"># set input to the graph module</span>
<span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_np</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y_np</span><span class="p">)</span>
<span class="c1"># run forward computation</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="c1"># get the first output</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="provide-model-parameters">
<h2>Provide Model Parameters<a class="headerlink" href="#provide-model-parameters" title="Permalink to this headline">¶</a></h2>
<p>Most deep learning models contains two types of inputs: parameters
that remains fixed during inference and data input that need to
change for each inference task. It is helpful to provide these
information to NNVM. Let us assume that <code class="docutils literal notranslate"><span class="pre">y</span></code> is the parameter
in our example. We can provide the model parameter information
by the params argument to <a class="reference internal" href="../../api/python/nnvm/compiler.html#nnvm.compiler.build" title="nnvm.compiler.build"><code class="xref any py py-func docutils literal notranslate"><span class="pre">nnvm.compiler.build</span></code></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">deploy_graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">nnvm</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
    <span class="n">compute_graph</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">shape</span><span class="p">},</span> <span class="n">params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="n">y_np</span><span class="p">})</span>
</pre></div>
</div>
<p>This time we will need params value returned by <a class="reference internal" href="../../api/python/nnvm/compiler.html#nnvm.compiler.build" title="nnvm.compiler.build"><code class="xref any py py-func docutils literal notranslate"><span class="pre">nnvm.compiler.build</span></code></a>.
NNVM applys  optimization  to pre-compute the intermediate values in
the graph that can be determined by parameters. In this case
<span class="math notranslate nohighlight">\(\sqrt{y}\)</span> can be pre-computed. The pre-computed values
are returned as new params. We can print out the new compiled library
to confirm that the fused kernel only now contains add.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-----optimized params-----&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-------deploy library-------&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lib</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="save-the-deployed-module">
<h2>Save the Deployed Module<a class="headerlink" href="#save-the-deployed-module" title="Permalink to this headline">¶</a></h2>
<p>We can save the <code class="docutils literal notranslate"><span class="pre">deploy_graph</span></code>, <code class="docutils literal notranslate"><span class="pre">lib</span></code> and <code class="docutils literal notranslate"><span class="pre">params</span></code> separately
and load them back later. We can use <a class="reference internal" href="../../api/python/module.html#tvm.module.Module" title="tvm.module.Module"><code class="xref any py py-class docutils literal notranslate"><span class="pre">tvm.module.Module</span></code></a> to export
the compiled library. <code class="docutils literal notranslate"><span class="pre">deploy_graph</span></code> is saved in json format and <code class="docutils literal notranslate"><span class="pre">params</span></code>
is serialized into a bytearray.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">temp</span> <span class="o">=</span> <a href="../../api/python/contrib.html#tvm.contrib.util.tempdir" title="tvm.contrib.util.tempdir" class="sphx-glr-backref-module-tvm-contrib-util sphx-glr-backref-type-py-function"><span class="n">util</span><span class="o">.</span><span class="n">tempdir</span></a><span class="p">()</span>
<span class="n">path_lib</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;deploy.so&quot;</span><span class="p">)</span>
<span class="n">lib</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">path_lib</span><span class="p">)</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;deploy.json&quot;</span><span class="p">),</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fo</span><span class="p">:</span>
    <span class="n">fo</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">deploy_graph</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;deploy.params&quot;</span><span class="p">),</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fo</span><span class="p">:</span>
    <span class="n">fo</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">nnvm</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">save_param_dict</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">listdir</span><span class="p">())</span>
</pre></div>
</div>
<p>We can load the module back.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">loaded_lib</span> <span class="o">=</span> <a href="../../api/python/module.html#tvm.module.load" title="tvm.module.load" class="sphx-glr-backref-module-tvm-module sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">path_lib</span><span class="p">)</span>
<span class="n">loaded_json</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;deploy.json&quot;</span><span class="p">))</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">loaded_params</span> <span class="o">=</span> <span class="nb">bytearray</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;deploy.params&quot;</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
<span class="n">module</span> <span class="o">=</span> <a href="../../api/python/graph_runtime.html#tvm.contrib.graph_runtime.create" title="tvm.contrib.graph_runtime.create" class="sphx-glr-backref-module-tvm-contrib-graph_runtime sphx-glr-backref-type-py-function"><span class="n">graph_runtime</span><span class="o">.</span><span class="n">create</span></a><span class="p">(</span><span class="n">loaded_json</span><span class="p">,</span> <span class="n">loaded_lib</span><span class="p">,</span> <a href="../../api/python/ndarray.html#tvm.gpu" title="tvm.gpu" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">gpu</span></a><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">nnvm</span><span class="o">.</span><span class="n">compiler</span><span class="o">.</span><span class="n">load_param_dict</span><span class="p">(</span><span class="n">loaded_params</span><span class="p">)</span>
<span class="c1"># directly load from byte array</span>
<span class="n">module</span><span class="o">.</span><span class="n">load_params</span><span class="p">(</span><span class="n">loaded_params</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x_np</span><span class="p">)</span>
<span class="c1"># get the first output</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">module</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="deploy-using-another-language">
<h2>Deploy using Another Language<a class="headerlink" href="#deploy-using-another-language" title="Permalink to this headline">¶</a></h2>
<p>We use python in this example for demonstration.
We can also deploy the compiled modules with other languages
supported by TVM such as  c++, java, javascript.
The graph module itself is fully embedded in TVM runtime.</p>
<p>The following block demonstrates how we can directly use TVM’s
runtime API to execute the compiled module.
You can find similar runtime API in TVMRuntime of other languages.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">fcreate</span> <span class="o">=</span> <a href="../../api/python/function.html#tvm.get_global_func" title="tvm.get_global_func" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span></a><span class="p">(</span><span class="s2">&quot;tvm.graph_runtime.create&quot;</span><span class="p">)</span>
<span class="n">ctx</span> <span class="o">=</span> <a href="../../api/python/ndarray.html#tvm.gpu" title="tvm.gpu" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">gpu</span></a><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gmodule</span> <span class="o">=</span> <span class="n">fcreate</span><span class="p">(</span><span class="n">loaded_json</span><span class="p">,</span> <span class="n">loaded_lib</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_type</span><span class="p">,</span> <span class="n">ctx</span><span class="o">.</span><span class="n">device_id</span><span class="p">)</span>
<span class="n">set_input</span><span class="p">,</span> <span class="n">get_output</span><span class="p">,</span> <span class="n">run</span> <span class="o">=</span> <span class="n">gmodule</span><span class="p">[</span><span class="s2">&quot;set_input&quot;</span><span class="p">],</span> <span class="n">gmodule</span><span class="p">[</span><span class="s2">&quot;get_output&quot;</span><span class="p">],</span> <span class="n">gmodule</span><span class="p">[</span><span class="s2">&quot;run&quot;</span><span class="p">]</span>
<span class="n">set_input</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">x_np</span><span class="p">))</span>
<span class="n">gmodule</span><span class="p">[</span><span class="s2">&quot;load_params&quot;</span><span class="p">](</span><span class="n">loaded_params</span><span class="p">)</span>
<span class="n">run</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">shape</span><span class="p">)</span>
<span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">asnumpy</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> ( 0 minutes  0.000 seconds)</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-nnvm-get-started-py">
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/4c1ad7c5229dacbaf05763d8058b0e7d/get_started.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">get_started.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/20a5fe4ed5bdda9ed0abdca12ff51e0c/get_started.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">get_started.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="from_mxnet.html" class="btn btn-neutral float-right" title="Compile MXNet Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="from_onnx.html" class="btn btn-neutral float-left" title="Compile ONNX Models" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2017, tvm developers

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>